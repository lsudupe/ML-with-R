---
title: 'Support Vector Machines'
author: Laura Sudupe Medinilla
subtitle: '`r params$subtitulo`'
date: '`r format(Sys.Date(),"%e de %B, %Y")`' 
# date: \today  (solo para pdf)
output:
  html_notebook:
    df_print: paged
    toc: yes
    toc_float: true
    theme: united
    highlight: tango
  pdf_document:
    keep_tex: yes
    toc: yes
    df_print: kable
    highlight: zenburn
  html_document:
    df_print: paged
    toc: yes
    toc_float: true
    theme: united
    highlight: tango
header-includes:
  - \usepackage[spanish]{babel}
params:
  file: colon2.csv
  p.train: 0.6666667
  subtitulo: Predict tumor type depending tissues genetic expression
  seed.train: 12345
  seed.clsfier: 1234567
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NULL, cache=TRUE)
options(width=90)
```


```{r packages, message=FALSE, echo=FALSE, warning=FALSE}
libraries <- c("ggplot2" ,"caret", "e1071")
check.libraries <- is.element(libraries, installed.packages()[, 1])==FALSE
libraries.to.install <- libraries[check.libraries]
if (length(libraries.to.install!=0)) {
  install.packages(libraries.to.install)
}

success <- sapply(libraries,require, quietly = FALSE,  character.only = TRUE)
if(length(success) != length(libraries)) {stop("A package failed to return a success in require() function.")}
```

\pagebreak


#Support Vector Machine (SVM)

A **Support Vector Machine (SVM)** can be imagined as a surface that creates a boundary between points of data plotted in multidimensional that represent examples and their feature values. The goal of a SVM is t create a flat boundary called **hyperplane**, which divides the space to create fairly homogeneus partitions on either side. In this way, the SVM learning combines aspects of both the instance-based nearest neighbor learning and the linear regression modeling. The combination is extremly powerful, allowing SVMs to model highly complex relationships.

| **Strengths**    | **Weaknesses**  | 
| ----------------------------------- |:-----------------------------------|
|- Can be used for classification or numeric prediction problems |- Finding the best model requires testing of various combinations of kernels and model parameters |
| - Not overly influenced by noisy data and not very prone to overfitting | - Can be slow to train, particularly if the input dataset has a large number of features or examples |
| - May be easier to use than neural networks, particularly due to the existence of several well-supported SVM algorithms | - Results in a complex black box model that is difficult, if not impossible, to interpret
| - Gaining popularity due to its high accuracy and high-profile wins in data mining competitions


# Predict tumor type depending tissues genetic expression

## Step 1 - Collecting the data

First, we are going to assign the variables and read the file.


```{r asigna, echo=TRUE, eval=FALSE}
file <- "colon2.csv"
p.train <- 0.6666667
subtitulo <- "Predict tumor type depending tissues genetic expression"
seed.train <- 1234

```


```{r frag1,message=FALSE,warning=FALSE}
dataset <- read.csv(file) 
length(complete.cases(dataset))
```

The file *`r params$file`* has `r nrow(dataset)` observations and `r ncol(dataset)` variables. The data doesn't need any transformation


## Step 2 - Exploring and preparing the data


In the `r ncol(dataset)` we have the type of tumor

```{r frag2, echo=FALSE}
table(dataset[,ncol(dataset)])
```

SVM learners require all features to be numeric, and moreover, that each feature is scaled to a fairly small interval. In this case, every feature is an integer, so we do not need to convert any factors into numbers


### Split the data in training/test

Let's separe the data in train and test.


```{r}

set.seed(params$valor.seed)
dataset$y <- as.factor(dataset$y)
train <- sample(nrow(dataset),floor(nrow(dataset)*params$p.train))

train_data <- dataset[train, ]
test_data <- dataset[-train, ]
```


## Step 3 - Train the models

We are going to use the package *kernlab* and train the linear and gaussian model

```{r , echo=FALSE,warning=FALSE,message=FALSE}


#train the linear model
lm <- ksvm( y ~ ., data = train_data, kernel = 'vanilladot')


#train the gaussian model
gs <- ksvm( y ~ ., data = train_data, kernel = 'rbfdot')
```



## Step 4 - Predicci?n y evaluaci?n del modelo

Una vez obtenido el primer modelo, se evalua su rendimiento con los datos de test. Se debe de clasificar las muestras test con la funci?n `compute`.

```{r frag13,warning=FALSE,message=FALSE,tidy=TRUE}
model_results_1 <- compute(data_model_1, data_nrm.test[,1:9])$net.result

# Put multiple binary output to categorical output
maxidx <- function(arr) {
  return(which(arr == max(arr)))
}

idx <- apply(model_results_1, 1, maxidx)
prediction <- c('malignant', 'benign')[idx]
res <- table(prediction, dataset$Class[-train] )

# Results
#require(caret)
(cmatrix1 <- confusionMatrix(res,positive="malignant"))
```

El modelo de una capa con categoria positiva 'malignant' obtiene una precisi?n de `r round(cmatrix1$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(cmatrix1$byClass["Sensitivity"], 3)` y `r round(cmatrix1$byClass["Specificity"], 3)` respectivamente.


##Step 5 - Mejora del rendimiento del modelo

El primer modelo fue con *un nodo* en la capa oculta. Ahora se plantea *3 nodos* en la capa oculta para tratar de mejorar el rendimiento.

```{r frag14,warning=FALSE,message=FALSE,tidy=TRUE,fig.height=4  }

# simple ANN with only a single hidden neuron
set.seed(params$seed.clsfier) # to guarantee repeatable results
data_model_3 <- neuralnet(fmla,
                          data = data_nrm.train,
                          hidden=3,linear.output=FALSE)

# visualize the network topology
plot(data_model_3, rep='best')

```

Tambi?n se puede hacer la representaci?n usando el paquete *NeuralNetTools*

```{r frag15,warning=FALSE,message=FALSE,tidy=TRUE,fig.height=4  }
# visualize the network topology with NeuralNetTools
plotnet(data_model_3,  prune_col = 'lightblue')
```

El resultado de la matriz de confusi?n con los datos de test es:


```{r frag16,warning=FALSE,message=FALSE,tidy=TRUE}
model_results_3 <- compute(data_model_3, data_nrm.test[,1:9])$net.result

# Put multiple binary output to categorical output
maxidx <- function(arr) {
  return(which(arr == max(arr)))
}

idx <- apply(model_results_3, 1, maxidx)
prediction <- c('malignant', 'benign')[idx]
res <- table(prediction, dataset$Class[-train] )

# Results
#require(caret)
(cmatrix3 <- confusionMatrix(res,positive="malignant"))
```


El nuevo modelo con 3 nodos ocultos obtiene una precisi?n de `r round(cmatrix3$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(cmatrix3$byClass["Sensitivity"], 3)` y `r round(cmatrix3$byClass["Specificity"], 3)` respectivamente. `r if(cmatrix1$overall["Accuracy"] > cmatrix3$overall["Accuracy"]){"Vemos que el modelo obtenido con un solo nodo tiene una mayor precisi?n"}``r if(cmatrix1$overall["Accuracy"] < cmatrix3$overall["Accuracy"]){"Vemos que el modelo obtenido con tres nodo tiene una mayor precisi?n"}``r if(cmatrix1$overall["Accuracy"] == cmatrix3$overall["Accuracy"]){"Vemos que ambos modelos tienen la misma precisi?n"}`

Al comparar los dos modelos si se ven resultados muy similares, se suele escoger el modelo m?s sencillo.

Hay que equilibrar la complejidad y la precisi?n del modelo, ya que modelos m?s complejos tambi?n son m?s susceptibles de tener overfitting.

## Paquete *caret*: modelo `nnet`

La funci?n `nnet` admite datos de tipo factor, as? que no hay que transformar la variable `Class` en variables binarias. 

Adem?s, se puede usar la funci?n `createDataPartition` para crear la partici?n de los datos en training/test.

```{r frag17,warning=FALSE,message=FALSE}
#library(nnet)

#Partici?n de datos
set.seed(params$seed.train)
# We wish 75% for the trainset 
inTrain <- createDataPartition(y=dataset$Class, p=0.66666666666666667, list=FALSE)
#dim(inTrain)
# Dataset normalizado
data_nrm <- cbind(data_nrm[,1:9],Class=dataset[,10])

train.set <- data_nrm[inTrain,]
test.set  <- data_nrm[-inTrain,]

#train.set <- dataset[inTrain,]
#test.set  <- dataset[-inTrain,]
nrow(train.set)/nrow(test.set) # should be around 2
```

**Train de 2/3 y 1/3 test**

```{r frag18,warning=FALSE,message=FALSE}
# modelo Train/test without repetition 
model <- train(Class ~ ., train.set, method='nnet', 
               trControl= trainControl(method='none'), 
              # preProcess = "range",
               tuneGrid= NULL, tuneLength=1 ,trace = FALSE) #

plotnet(model)
summary(model)
prediction <- predict(model, test.set[-10])                           # predict
table(prediction, test.set$Class)                                  # compare

# predict can also return the probability for each class:
prediction <- predict(model, test.set[-10], type="prob")  
head(prediction)
```

**5-fold crossvalidation**

```{r frag19,warning=FALSE,message=FALSE,fig.height=4}
# modelo 5-crossvalidation 
model <- train(Class ~ ., train.set, method='nnet', 
               trControl= trainControl(method='cv', number=5), 
               tuneGrid= NULL, tuneLength=10 ,trace = FALSE)

plotnet(model, alpha=0.6)
summary(model)
prediction <- predict(model, test.set[-10])                           # predict
table(prediction, test.set$Class)                                  # compare

# predict can also return the probability for each class:
prediction <- predict(model, test.set[-10], type="prob")  
head(prediction)
```


**Bootstrap**

```{r frag20,warning=FALSE,message=FALSE,fig.height=4}
# Por defecto es Bootstrap, con 25 repeticiones para 3 posibles decay
# y 3 posibles sizes
model <- train(Class ~ ., train.set, method='nnet', trace = FALSE) # train
# we also add parameter 'preProc = c("center", "scale"))' at train() for centering and scaling the data

plotnet(model)
summary(model)
prediction <- predict(model, test.set[-10])                           # predict
table(prediction, test.set$Class)                                  # compare

# predict can also return the probability for each class:
prediction <- predict(model, test.set[-10], type="prob")  
head(prediction)
```


#Referencias
