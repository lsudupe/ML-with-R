---
title: 'Support Vector Machines'
author: Laura Sudupe Medinilla
subtitle: '`r params$subtitulo`'
date: '`r format(Sys.Date(),"%e de %B, %Y")`' 
# date: \today  (solo para pdf)
output:
  html_notebook:
    df_print: paged
    toc: yes
    toc_float: true
    theme: united
    highlight: tango
  pdf_document:
    keep_tex: yes
    toc: yes
    df_print: kable
    highlight: zenburn
  html_document:
    df_print: paged
    toc: yes
    toc_float: true
    theme: united
    highlight: tango
header-includes:
  - \usepackage[spanish]{babel}
params:
  file: colon2.csv
  p.train: 0.6666667
  subtitulo: Predict tumor type depending tissues genetic expression
  seed.train: 12345
  seed.clsfier: 1234567
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NULL, cache=TRUE)
options(width=90)
```


```{r packages, message=FALSE, echo=FALSE, warning=FALSE}
libraries <- c("ggplot2" ,"caret", "e1071")
check.libraries <- is.element(libraries, installed.packages()[, 1])==FALSE
libraries.to.install <- libraries[check.libraries]
if (length(libraries.to.install!=0)) {
  install.packages(libraries.to.install)
}

success <- sapply(libraries,require, quietly = FALSE,  character.only = TRUE)
if(length(success) != length(libraries)) {stop("A package failed to return a success in require() function.")}
```

\pagebreak


#Support Vector Machine (SVM)

A **Support Vector Machine (SVM)** can be imagined as a surface that creates a boundary between points of data plotted in multidimensional that represent examples and their feature values. The goal of a SVM is t create a flat boundary called **hyperplane**, which divides the space to create fairly homogeneus partitions on either side. In this way, the SVM learning combines aspects of both the instance-based nearest neighbor learning and the linear regression modeling. The combination is extremly powerful, allowing SVMs to model highly complex relationships.

| **Strengths**    | **Weaknesses**  | 
| ----------------------------------- |:-----------------------------------|
|- Can be used for classification or numeric prediction problems |- Finding the best model requires testing of various combinations of kernels and model parameters |
| - Not overly influenced by noisy data and not very prone to overfitting | - Can be slow to train, particularly if the input dataset has a large number of features or examples |
| - May be easier to use than neural networks, particularly due to the existence of several well-supported SVM algorithms | - Results in a complex black box model that is difficult, if not impossible, to interpret
| - Gaining popularity due to its high accuracy and high-profile wins in data mining competitions


# Predict tumor type depending tissues genetic expression

## Step 1 - Collecting the data

First, we are going to assign the variables and read the file.


```{r asigna, echo=TRUE, eval=FALSE}
file <- "colon2.csv"
p.train <- 0.6666667
subtitulo <- "Predict tumor type depending tissues genetic expression"
seed.train <- 1234

```


```{r frag1,message=FALSE,warning=FALSE}
dataset <- read.csv(file) 
length(complete.cases(dataset))
```

The file *`r params$file`* has `r nrow(dataset)` observations and `r ncol(dataset)` variables. The data doesn't need any transformation


## Step 2 - Exploring and preparing the data


In the `r ncol(dataset)` we have the type of tumor

```{r frag2, echo=FALSE}
table(dataset[,ncol(dataset)])
```

SVM learners require all features to be numeric, and moreover, that each feature is scaled to a fairly small interval. In this case, every feature is an integer, so we do not need to convert any factors into numbers


### Split the data in training/test

Let's separe the data in train and test.


```{r}

set.seed(params$valor.seed)
dataset$y <- as.factor(dataset$y)
train <- sample(nrow(dataset),floor(nrow(dataset)*params$p.train))

train_data <- dataset[train, ]
test_data <- dataset[-train, ]
```


## Step 3 - Train the models

We are going to use the package *kernlab* and train the linear and gaussian model

```{r , echo=FALSE,warning=FALSE,message=FALSE}


#train the linear model
lm <- ksvm( y ~ ., data = train_data, kernel = 'vanilladot')

#train the gaussian model
gs <- ksvm( y ~ ., data = train_data, kernel = 'rbfdot')

```



## Step 4 - Evaluating model performance

The `predict()` function allows us to use the classification model to make predictions on the testing dataset.

```{r ,warning=FALSE,message=FALSE,tidy=TRUE}

lm_pre <- predict(lm, test_data)
gs_pre <- predict(gs, test_data)

```


```{r}
lm_pre
```


To examine how well our classifier performed, we will use `confusionMatrix()`.

```{r}

lm_cm <- confusionMatrix(lm_pre, test_data$y, positive = "t")
gs_cm <- confusionMatrix(gs_pre, test_data$y, positive = "t")

```

El modelo de una capa con categoria positiva 'malignant' obtiene una precisi?n de `r round(cmatrix1$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(cmatrix1$byClass["Sensitivity"], 3)` y `r round(cmatrix1$byClass["Specificity"], 3)` respectivamente.

```{r}
lm_cm
```

```{r}
gs_cm
```


