---
title: 'Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks'
author: Laura Sudupe Medinilla
subtitle: '`r params$subtitulo`'
date: '`r format(Sys.Date(),"%e de %B, %Y")`' 
# date: \today  (solo para pdf)
output:
  pdf_document:
    keep_tex: yes
    toc: yes
    df_print: kable
    highlight: zenburn
  html_document:
    df_print: paged
    toc: yes
    toc_float: true
    theme: united
    highlight: tango
  html_notebook:
    df_print: paged
    toc: yes
    toc_float: true
    theme: united
    highlight: tango
header-includes:
  - \usepackage[spanish]{babel}
params:
  file: pcaComponents7.csv
  p.train: 0.6666667
  subtitulo: PEC2
  seed.train: 12345
  seed.clsfier: 1234567
---

```{r setup, include=FALSE}
#With this chunck when we run the scripts it creates folders to save the results. When we run again the script only run the changed code. Good option when our script has a high computational cost
knitr::opts_chunk$set(echo = TRUE, comment = NULL, cache=TRUE)
options(width=90)
```


```{r packages, message=FALSE, echo=FALSE, warning=FALSE}
libraries <- c("ggplot2" ,"caret", "e1071", "kernlab", "neuralnet", "NeuralNetTools")
check.libraries <- is.element(libraries, installed.packages()[, 1])==FALSE
libraries.to.install <- libraries[check.libraries]
if (length(libraries.to.install!=0)) {
  install.packages(libraries.to.install)
}

success <- sapply(libraries,require, quietly = FALSE,  character.only = TRUE)
if(length(success) != length(libraries)) {stop("A package failed to return a success in require() function.")}
```



# Objetive

In this PEC we are going to do a dinamic report where we are going to analize an experiment related with the prediction of four different cancer types and healthy patients.
For these purpose, we are going to analize the data with the implementation of two different algorithms: `Artificial Neural Network` and `Support Vector Machine` to predict the type.



# Introduction

The first and second step are the same for both algorithms so we are going to do them in the first place. After this, we are going to explain briefly both algorithms.We are going to add a table with their strenghs and weaknesess to understand better the posibilities. Inside the algorithmns part, we are going to process with three different steps:

- Model training with the data
- Model performance evaluation
- Model improvement

In both cases, we are going to analyze the algorithms quality and performance with the `confusionMatrix()` function from **caret** package. At the end, we are going to add a conclusion and discussion section.


## Step 1 - Collecting the data 

First of all, we are going to import the files we are going to need. In one hand, we have the `file` file where we have our data PCA analysis values, we are going to use the first 10 principal components for our analysis. In the `file3` we have our genetic expression data without changes. To finish, we are going to import the `file2` file where we have annotated the different cancer types. 

```{r, message=FALSE,warning=FALSE}

# import the csv
file <- "pcaComponents7.csv"
file2 <- "class7.csv"
file3 <- "data7.csv"

pca_comp <- read.csv(file, header=TRUE)
pca_10comp <- pca_comp[,1:10]

tumor_type <- read.csv(file2, header=TRUE)
data_original <- read.csv(file3, header = TRUE)

#prepare our dataframes
data <- cbind.data.frame(pca_10comp, tumor_type)
data_svm <- cbind.data.frame(data_original, tumor_type)

```

## Step 2- Exploring and preparing the data 

With the `str()` command, we observated the data in both cases. In the first case, we have `r nrow(data)` raws and `r ncol(data)` columns. In the second dataset, we have `r nrow(data_svm)` raws and `r ncol(data_svm)` columns.


```{r, echo=FALSE}
# examinando la estructura del dataset
str(data)
str(data_svm)
```

A lot of machine learning classifiers need the target variable to be a factor, so we codified our data.

```{r}
#We are going to change the "x" column type and name
data$x <- factor(data$x, levels = c(1,2,3,4,5),
                         labels = c("ALL", "AML", "CLL", "CML", "NoL"))

data_svm$x <- factor(data_svm$x, levels = c(1,2,3,4,5),
                         labels = c("ALL", "AML", "CLL", "CML", "NoL"))

#change in the both cases the "x" column name to "type"
names(data)[11] <- "type"
names(data_svm)[ncol(data_svm)] <- "type"

```

Let's comprobate

```{r}
class(data$type)
```
We can check the number of tumor types in each case

```{r}
table(data$type)
```


### Transformacion - Normalization

In both cases, we need to normalize our data, in the case of SVM, the R package we will use for fitting the model will perform the rescaling automaticaly, instead no in the neural network model.

```{r}

#Our normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}

data_nrm <- as.data.frame(lapply(data[,-11],normalize))
summary(data_nrm)

```

### Binary variable creation instead of the factor class variables

The `neuralnet()` function we will use for the neural network algorithm doesn't accept factor or categorical variables. In this case, we need to transform our `type` variable to binary type. We are going to create four five new variables acording to the cancer type with TRUE and FALSE values.


```{r}
# Binary variables creation 

data_nrm$ALL <- data$type=="ALL"
data_nrm$AML <- data$type=="AML"
data_nrm$CLL <- data$type=="CLL"
data_nrm$CML <- data$type=="CML"
data_nrm$NoL <- data$type=="NoL"


```

In the case of the data for the SVM algorithm, it requires all features to be numeric. In this case, every feature is an intefer, so we do not need to convert any factor into numbers

### Creating training and test datasets

To generate the train and test data, we split both datasets in two parts. We will obtain to training data and two test data. We will extract the data randomly with `r round(params$p.train *100,2)`% of the observations

```{r}
# create training and test data
set.seed(params$seed.train)

train <- sample(nrow(data_nrm),floor(nrow(data_nrm)*params$p.train))
data_nrm.train <- data_nrm[train,]
data_nrm.test  <- data_nrm[-train,]

# create training and test data SVM

data_svm$type <- as.factor(data_svm$type)
train_svm <- sample(nrow(data_svm),floor(nrow(data_svm)*params$p.train))
data_svm.train <- data_svm[train_svm,]
data_svm.test  <- data_svm[-train_svm,]

```


# Neural Networks algorithm (ANN)

# Naive Bayes Algorithm 

The *Artificial Neural Network (ANN)* models the relationship between a set of input signals and an output signal using a model derived from our understanding of how a biological brain responds to stimuli from sensory inputs. Just as a brain uses a network of interconnected cells called neurons to create a massive parallel processor, ANN uses a network of artificial neurons or nodes to solve learning problems. 


| **Strenghts**    | **Weaknesses**  | 
| ----------------------------- |:------------------------------------------- |
| * Can be adapted to classification or numeric prediction problems | * Extremely computationally intensive and slow to train, particularly if the network topology is complex|
| * Capable of modeling more complex patterns than nearly any algorithm  |  *Very prone to overfitting training data |
| * Makes few assumptions about the data's underlying relationships  | * Results in a complex black box model that is difficult, if not impossible, to interpret |  |



## Step 3 - Training a model on the data

We will use our normalized and binarized data. For the neural  network constructions we will use the `neuralnet()` function for the *neuralnet* package. 
The model formula has `r ncol(data_nrm)` input nodes and `nlevels(data$type)` output nodes:


```{r}
# we will create a formula for high variables dataset
xnam <- names(data_nrm[1:10])
(fmla <- as.formula(paste("ALL + AML + CLL + CML + NoL ~ ", paste(xnam, collapse= "+"))))
```

The moddel have one intermediate node, selected with `hidden=1`.

```{r}
set.seed(1234567)
ANN1  <- neuralnet(fmla,
                   data_nrm.train,
                   hidden=1,
                   linear.output=FALSE)
```

```{r}
# model representation
plot(ANN1,rep="best")  
```

Same model representation with *NeuralNetTools* package

```{r}
plotnet(ANN1, alpha = 0.6)
```

## Step 4 - Evaluating model performance

Once we obtain the model, let´s classify the test data with `compute` function

```{r}
ANN1_results <- neuralnet::compute(ANN1, data_nrm.test[,1:10])$net.result

# Put multiple binary output to categorical output
maxidx <- function(arr) {
  return(which(arr == max(arr)))
}

idx <- apply(ANN1_results, 1, maxidx)
prediction1 <- c("ALL", "AML", "CLL", "CML", "NoL")[idx]
res1 <- table(prediction1, data$type[-train] )

# Results
#require(caret)
(cmatrix1 <- confusionMatrix(res1))


```

Using the neuronal network of one hidden node, we obtain `r sum(matrix1$t)`, with `r matrix$t[1,1]` true positives, `r matrix$t[2,2]` true negatives, `r matrix$t[2,1]` false negatives and `r matrix$t[1,2]` false positives. With this algorithm, we obtain `r round(matrix$overall["Accuracy"], 3)` precision and a `r round(matrix$overall["Kappa"], 7)` kappa value.

Los valores de sensibilidad y especificidad son de 0.8824 y 1, respectivamente. Este algoritmo clasifica bien 33 de 35 secuencias, mientras que el numero de falsos negativos (2) es mayor que el de falsos positivos, puesto que no hay. 

## Step 5 - Model improvement

We are going to try to improve the model with five hidden nodes. The model formula has `r ncol(data_nrm)` input nodes and `nlevels(data$type)` output nodes:

```{r}
set.seed(1234567)
ANN5  <- neuralnet(fmla,
                   data_nrm.train,
                   hidden=5,
                   linear.output=FALSE)
```

Let's see the representation with *NeuralNetTools* package:

```{r}
plotnet(ANN5, alpha = 0.6)
```

We are going to classify our test data with the `compute` function

```{r}
set.seed(1234567)

ANN5_results <- neuralnet::compute(ANN5, data_nrm.test[,1:10])$net.result

# Put multiple binary output to categorical output
maxidx <- function(arr) {
  return(which(arr == max(arr)))
}

idx <- apply(ANN5_results, 1, maxidx)
prediction5 <- c("ALL", "AML", "CLL", "CML", "NoL")[idx]
res5 <- table(prediction5, data$type[-train] )

# Results
#require(caret)
(cmatrix5 <- confusionMatrix(res5))


```

Utilizando una red neuronal de cinco capas ocultas con categoria positiva 'P', de un total de `r sum(matrix5$t)`, hay `r matrix5$t[1,1]` verdaderos positivos, `r matrix5$t[2,2]` verdaderos negativos, `r matrix5$t[2,1]` falsos negativos  y `r matrix5$t[1,2]` falsos positivos.  Este algoritmo, por tanto, consigue una precision de `r round(matrix5$overall["Accuracy"], 3)` y un estadístico kappa de `r round(matrix5$overall["Kappa"], 7)`. Los valores de sensibilidad y especificidad son de 0.8235 y 0.8889, respectivamente. Este algoritmo clasifica bien 30 de 35 secuencias, mientras que el numero de falsos negativos (3) es mayor que el de falsos positivos (2). En terminos generales, podemos ver que la red neuronal con cinco capas ocultas parece funcionar peor que la de cuatro capas ocultas.

La red neuronal de cuatro capas ocultas es el mejores clasificador de secuencias promotoras hasta ahora.


## *caret* packagev: `nnet` model

With the `nnet` function, we don't need to transdorme our target data to binary type. Also, we can use `createDataPartition` function to split the data in train and test.

```{r}
library(nnet)

set.seed(params$seed.train)
# We wish 75% for the trainset 
inTrain <- createDataPartition(y=data$type, p=0.66666666666666667, list=FALSE)
#dim(inTrain)

# normalized dataset
data_nrm_net <- cbind(data_nrm[,1:10],type=data[,11])

train.set_net <- data_nrm_net[inTrain,]
test.set_net  <- data_nrm_net[-inTrain,]

nrow(train.set_net)/nrow(test.set_net) # should be around 2
```


**3-fold crossvalidation**

Also, we are going to check the performance with 3-fold crossvalidation

```{r}

set.seed(1234567)

#3-crossvalidation model
model_net3_cross <- train(type ~ ., train.set_net, method='nnet', 
               trControl= trainControl(method='cv', number=3), 
               tuneGrid= NULL, tuneLength=10 ,trace = FALSE)

plotnet(model_net3_cross, alpha=0.6)
summary(model_net3_cross)
prediction_net3_cross <- predict(model_net3_cross, test.set_net[-11])   # predict
table(prediction_net3_cross, test.set_net$type)                         # compare

# predict can also return the probability for each class:
prediction_net3_cross_pro <- predict(model_net3_cross, test.set_net[-11], type="prob")  
head(prediction_net3_cross_pro)
```



# Support Vector Machine (SVM)

A **Support Vector Machine (SVM)** can be imagined as a surface that creates a boundary between points of data plotted in multidimensional that represent examples and their feature values. The goal of a SVM is t create a flat boundary called **hyperplane**, which divides the space to create fairly homogeneus partitions on either side. In this way, the SVM learning combines aspects of both the instance-based nearest neighbor learning and the linear regression modeling. The combination is extremly powerful, allowing SVMs to model highly complex relationships.

| **Strengths**    | **Weaknesses**  | 
| ----------------------------------- |:-----------------------------------|
|- Can be used for classification or numeric prediction problems |- Finding the best model requires testing of various combinations of kernels and model parameters |
| - Not overly influenced by noisy data and not very prone to overfitting | - Can be slow to train, particularly if the input dataset has a large number of features or examples |
| - May be easier to use than neural networks, particularly due to the existence of several well-supported SVM algorithms | - Results in a complex black box model that is difficult, if not impossible, to interpret
| - Gaining popularity due to its high accuracy and high-profile wins in data mining competitions

## Step 3 - Training a model on the data

We are going to use the  `ksvm()` function from `kernlab` package to train the SVM model. We are going to build a linear model with the kernel value `vanilladot`


```{r}
set.seed(1234567)

lm <- ksvm( type ~ ., data = data_svm.train,kernel = "vanilladot")
lm
```

## Step 4 - Evaluating model performance

The `predict()` function allows us to use the classification model to make predictions on the testing dataset `data_svm.test`


```{r}
pre_lm <- predict(lm, data_svm.test)
pre_lm
```

Let's see the model performance with `ConfusionMatrix()` function

```{r}
cm_lm <- confusionMatrix(pre_lm, data_svm.test$type)
cm_lm
```

Utilizando un modelo SVM lineal con categoria positiva 'P', de un total de `r sum(cm$t)`, hay `r cm$t[1,1]` verdaderos positivos, `r cm$t[2,2]` verdaderos negativos, `r cm$t[2,1]` falsos negativos  y `r cm$t[1,2]` falsos positivos.  Este algoritmo, por tanto, consigue una precision de `r round(cm$overall["Accuracy"], 3)` y un estadístico kappa de `r round(cm$overall["Kappa"], 7)`. Los valores de sensibilidad y especificidad son de 0.9412 y 0.8333, respectivamente. Este algoritmo clasifica bien 31 de 35 secuencias, mientras que el numero de falsos positivos (3) es mayor que el de falsos negativos (1). 


## Step 5 - Improvement the model

We are going to do the same but with a Gaussian kernel `rbfdot` to try to improve the model performance

```{r}
set.seed(1234567)

gs <- ksvm(type ~ ., data = data_svm.train, kernel='rbfdot')
gs
```

```{r}
pre_gs <- predict(gs, data_svm.test)
pre_gs
```

```{r}
cm_gs <- confusionMatrix(pre_gs, data_svm.test$type)
cm_gs
```

Utilizando un modelo SVM gausiano con categoria positiva 'P', de un total de `r sum(cm_g$t)`, hay `r cm_g$t[1,1]` verdaderos positivos, `r cm_g$t[2,2]` verdaderos negativos, `r cm_g$t[2,1]` falsos negativos  y `r cm_g$t[1,2]` falsos positivos.  Este algoritmo, por tanto, consigue una precision de `r round(cm_g$overall["Accuracy"], 3)` y un estadístico kappa de `r round(cm_g$overall["Kappa"], 7)`. Los valores de sensibilidad y especificidad son de 0.9412 y 0.9444, respectivamente. Este algoritmo clasifica bien 33 de 35 secuencias, mientras que el numero de falsos positivos y falsos negativos es bajo, tan solo 1 por cada uno. En terminos de eficiencia, el modelo ha mejorado con respecto al SVM lineal, puesto que clasifica bien 2 secuencias mas. Tambien aumenta la especificidad utilizando este algoritmo con respecto al lineal.

En terminos generales, ambos modelos son bastantes eficientes, pero parece que el SVM gausiano funciona mejor con estos datos. 

Hasta ahora, podemos ver que tanto el SVM gausiano como la red neuronal de cuatro capas ocultas obtienen las mejores eficiencias al clasificar las secuencias (promotoras o no). La diferencia entre ellos es que el SVM gausiano obtiene 1 falso negativo y 1 falso positivo, mientras que la red neuronal de cuatro capas ocultas obtiene dos falsos negativos y ningun falso positivo.




# Discussion 



De acuerdo a las matrices de confusion, en las que siempre se ha otorgado a la secuencia promotora la categoria positiva, en general parece estar claro que, si nos centramos en la precision, el algoritmo de clasificacion que mejor funciona para el caso de estudio son los arboles de decision con boosting = 1, ya que la precision es maxima, clasificando en la clase "real" cada secuencia (promotora versus no promotora). Los arboles de decision con boosting=10 tambien obtienen una precision de 1, pero tan solo utilizaron una interaccion, por lo que las otras 9 no fueron necesarias. Tras este, obteniendo una precision de 0.9714, el modelo Random Forests con 100 arboles seria el siguiente mejor clasificador de secuencias. Tras estos dos, con una precision de 0.9429 se encuentran los algoritmos Random Forests con 50 arboles, el SVM gausiano y la red neuronal de 4 capas ocultas. Random Forests con 50 arboles y el SVM gausiano comparten que obtienen 1 falso negativo y 1 falso positivo, mientras que la red neuronal de cuatro capas ocultas obtiene dos falsos negativos y ningun falso positivo. A continuacion, con una precision de 0.9143 aparecen los algoritmos Naive Bayes, tanto con lapalce=0 como con laplace=1, ya que ambos clasifican bien 32 de 35 secuencias, mientras que el numero de falsos negativos es de 2 y el de falsos positivos de 1. Al utilizar el algoritmo SVM lineal se obtiene una precision de 0.8857, que si bien es bastante alta, nos demuestra que en este caso, si tuviesemos que escoger entre los dos SVM utilizados, escogeriamos el SVM gausiano. Lo mismo pasa con las redes neuronales, ya que al utilizar cinco capas ocultas obtenemos una precision de 0.8571, que al comparar con el resultado obtenido al utilizar cuatro capas ocultas nos indica que es mas aconsejable utilizar el modelo con menos capas. Por ultimo, aparecen los algoritmos k-NN, siendo el numero de vecinos utilizados mas optimo, el de 3, puesto que obtenemos una precision de 0.8. Utilizando menos vecinos (k=1), la precision baja a 0.7714, lo mismo que ocurre al aumentar los vecinos (con k=5 obtenemos una precision de 0.7429 y con k=7 de 0.7714).

Con estos datos, podemos afirmar que los algoritmo k-NN son los menos optimos para clasificar secuencias promotoras con los datos del estudio, si bien es cierto que la precision no es extremadamente baja. El resto de clasificadores obtienen una precision bastante alta, pero sin duda, parece que el mejor algoritmo de clasificacion es el de arboles de decision, ya que parecen no cometer ningun error en las predicciones.Por supuesto, harian falta mas medidas de rendimiento para confirmar estos resultados, ya que tan solo con la eficiencia (o incluso con sensibilidad y especificidad) podemos estar perdiendonos informacion esecial y sobrevalorar o infravalorar un clasificador.


