---
title: 'Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks'
author: Laura Sudupe Medinilla
subtitle: '`r params$subtitulo`'
date: '`r format(Sys.Date(),"%e de %B, %Y")`' 
# date: \today  (solo para pdf)
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float: true
    theme: united
    highlight: tango
  pdf_document:
    keep_tex: yes
    toc: yes
    df_print: kable
    highlight: zenburn
  html_notebook:
    df_print: paged
    toc: yes
    toc_float: true
    theme: united
    highlight: tango
header-includes:
  - \usepackage[spanish]{babel}
params:
  file: pcaComponents7.csv
  p.train: 0.6666667
  subtitulo: PEC2
  seed.train: 12345
  seed.clsfier: 1234567
---

```{r setup, include=FALSE}
#With this chunck when we run the scripts it creates folders to save the results. When we run again the script only run the changed code. Good option when our script has a high computational cost
knitr::opts_chunk$set(echo = TRUE, comment = NULL, cache=TRUE)
options(width=90)
```


```{r packages, message=FALSE, echo=FALSE, warning=FALSE}
libraries <- c("ggplot2" ,"caret", "e1071", "kernlab", "neuralnet", "NeuralNetTools")
check.libraries <- is.element(libraries, installed.packages()[, 1])==FALSE
libraries.to.install <- libraries[check.libraries]
if (length(libraries.to.install!=0)) {
  install.packages(libraries.to.install)
}

success <- sapply(libraries,require, quietly = FALSE,  character.only = TRUE)
if(length(success) != length(libraries)) {stop("A package failed to return a success in require() function.")}
```



# Objetive

In this PEC we are going to do a dinamic report where we are going to analize an experiment related with the prediction of four different cancer types and healthy patients.
For these purpose, we are going to analize the data with the implementation of two different algorithms: `Artificial Neural Network` and `Support Vector Machine` to predict the type.



# Introduction

The first and second step are the same for both algorithms so we are going to do them in the first place. After this, we are going to explain briefly both algorithms.We are going to add a table with their strenghs and weaknesess to understand better the posibilities. Inside the algorithmns part, we are going to process with three different steps:

- Model training with the data
- Model performance evaluation
- Model improvement

In both cases, we are going to analyze the algorithms quality and performance with the `confusionMatrix()` function from **caret** package. At the end, we are going to add a conclusion and discussion section.


## Step 1 - Collecting the data 

First of all, we are going to import the files we are going to need. In one hand, we have the `file` file where we have our data PCA analysis values, we are going to use the first 10 principal components for our analysis. In the `file3` we have our genetic expression data without changes. To finish, we are going to import the `file2` file where we have annotated the different cancer types. 

```{r, message=FALSE,warning=FALSE}

# import the csv
file <- "pcaComponents7.csv"
file2 <- "class7.csv"
file3 <- "data7.csv"

pca_comp <- read.csv(file, header=TRUE)
pca_10comp <- pca_comp[,1:10]

tumor_type <- read.csv(file2, header=TRUE)
data_original <- read.csv(file3, header = TRUE)

#prepare our dataframes
data <- cbind.data.frame(pca_10comp, tumor_type)
data_svm <- cbind.data.frame(data_original, tumor_type)

```

## Step 2- Exploring and preparing the data 

With the `str()` command, we observated the data in both cases. In the first case, we have `r nrow(data)` raws and `r ncol(data)` columns. In the second dataset, we have `r nrow(data_svm)` raws and `r ncol(data_svm)` columns.


```{r, echo=FALSE}
# examinando la estructura del dataset
str(data)
str(data_svm)
```

A lot of machine learning classifiers need the target variable to be a factor, so we codified our data.

```{r}
#We are going to change the "x" column type and name
data$x <- factor(data$x, levels = c(1,2,3,4,5),
                         labels = c("ALL", "AML", "CLL", "CML", "NoL"))

data_svm$x <- factor(data_svm$x, levels = c(1,2,3,4,5),
                         labels = c("ALL", "AML", "CLL", "CML", "NoL"))

#change in the both cases the "x" column name to "type"
names(data)[11] <- "type"
names(data_svm)[ncol(data_svm)] <- "type"

```

Let's comprobate

```{r}
class(data$type)
```
We can check the number of tumor types in each case

```{r}
table(data$type)
```


### Transformacion - Normalization

In both cases, we need to normalize our data, in the case of SVM, the R package we will use for fitting the model will perform the rescaling automaticaly, instead no in the neural network model.

```{r}

#Our normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}

data_nrm <- as.data.frame(lapply(data[,-11],normalize))
summary(data_nrm)

```

### Binary variable creation instead of the factor class variables

The `neuralnet()` function we will use for the neural network algorithm doesn't accept factor or categorical variables. In this case, we need to transform our `type` variable to binary type. We are going to create four five new variables acording to the cancer type with TRUE and FALSE values.


```{r}
# Binary variables creation 

data_nrm$ALL <- data$type=="ALL"
data_nrm$AML <- data$type=="AML"
data_nrm$CLL <- data$type=="CLL"
data_nrm$CML <- data$type=="CML"
data_nrm$NoL <- data$type=="NoL"


```

In the case of the data for the SVM algorithm, it requires all features to be numeric. In this case, every feature is an intefer, so we do not need to convert any factor into numbers

### Creating training and test datasets

To generate the train and test data, we split both datasets in two parts. We will obtain to training data and two test data. We will extract the data randomly with `r round(params$p.train *100,2)`% of the observations

```{r}
# create training and test data
set.seed(params$seed.train)

train <- sample(nrow(data_nrm),floor(nrow(data_nrm)*params$p.train))
data_nrm.train <- data_nrm[train,]
data_nrm.test  <- data_nrm[-train,]

# create training and test data SVM

data_svm$type <- as.factor(data_svm$type)
train_svm <- sample(nrow(data_svm),floor(nrow(data_svm)*params$p.train))
data_svm.train <- data_svm[train_svm,]
data_svm.test  <- data_svm[-train_svm,]

```


# Neural Networks algorithm (ANN)

# Naive Bayes Algorithm 

The *Artificial Neural Network (ANN)* models the relationship between a set of input signals and an output signal using a model derived from our understanding of how a biological brain responds to stimuli from sensory inputs. Just as a brain uses a network of interconnected cells called neurons to create a massive parallel processor, ANN uses a network of artificial neurons or nodes to solve learning problems. 


| **Strenghts**    | **Weaknesses**  | 
| ----------------------------- |:------------------------------------------- |
| * Can be adapted to classification or numeric prediction problems | * Extremely computationally intensive and slow to train, particularly if the network topology is complex|
| * Capable of modeling more complex patterns than nearly any algorithm  |  *Very prone to overfitting training data |
| * Makes few assumptions about the data's underlying relationships  | * Results in a complex black box model that is difficult, if not impossible, to interpret |  |



## Step 3 - Training a model on the data

We will use our normalized and binarized data. For the neural  network constructions we will use the `neuralnet()` function for the *neuralnet* package. 
The model formula has `r ncol(data_nrm)` input nodes and `nlevels(data$type)` output nodes:


```{r}
# we will create a formula for high variables dataset
xnam <- names(data_nrm[1:10])
(fmla <- as.formula(paste("ALL + AML + CLL + CML + NoL ~ ", paste(xnam, collapse= "+"))))
```

The moddel have one intermediate node, selected with `hidden=1`.

```{r}
set.seed(1234567)
ANN1  <- neuralnet(fmla,
                   data_nrm.train,
                   hidden=1,
                   linear.output=FALSE)
```

```{r}
# model representation
plot(ANN1,rep="best")  
```

Same model representation with *NeuralNetTools* package

```{r}
plotnet(ANN1, alpha = 0.6)
```

## Step 4 - Evaluating model performance

Once we obtain the model, letÂ´s classify the test data with `compute` function

```{r}
ANN1_results <- neuralnet::compute(ANN1, data_nrm.test[,1:10])$net.result

# Put multiple binary output to categorical output
maxidx <- function(arr) {
  return(which(arr == max(arr)))
}

idx <- apply(ANN1_results, 1, maxidx)
prediction1 <- c("ALL", "AML", "CLL", "CML", "NoL")[idx]
res1 <- table(prediction1, data$type[-train] )

# Results
#require(caret)
(cmatrix1 <- confusionMatrix(res1))


```
Using the neuronal network of one hidden node, we obtain `r sum(cmatrix1$t)`, with `r cmatrix1$t[1,1]` true positives in the case of ALL for example. With this algorithm, we obtain `r round(cmatrix1[["overall"]][["Accuracy"]], 3)`precision and a `r round(cmatrix1[["overall"]][["Kappa"]], 7)` kappa value.

 

## Step 5 - Model improvement

We are going to try to improve the model with five hidden nodes. The model formula has `r ncol(data_nrm)` input nodes and `nlevels(data$type)` output nodes:

```{r}
set.seed(1234567)
ANN5  <- neuralnet(fmla,
                   data_nrm.train,
                   hidden=5,
                   linear.output=FALSE)
```

Let's see the representation with *NeuralNetTools* package:

```{r}
plotnet(ANN5, alpha = 0.6)
```

We are going to classify our test data with the `compute` function

```{r}
set.seed(1234567)

ANN5_results <- neuralnet::compute(ANN5, data_nrm.test[,1:10])$net.result

# Put multiple binary output to categorical output
maxidx <- function(arr) {
  return(which(arr == max(arr)))
}

idx <- apply(ANN5_results, 1, maxidx)
prediction5 <- c("ALL", "AML", "CLL", "CML", "NoL")[idx]
res5 <- table(prediction5, data$type[-train] )

# Results
#require(caret)
(cmatrix5 <- confusionMatrix(res5))


```


Using the neuronal network of one hidden node, we obtain `r sum(cmatrix5$t)`, with `r cmatrix5$t[1,1]` true positives in the case of ALL for example. With this algorithm, we obtain `r round(cmatrix5[["overall"]][["Accuracy"]], 3)`precision and a `r round(cmatrix5[["overall"]][["Kappa"]], 7)` kappa value.

We have a better values of sensibility and specifity with the neural network with five hidden nodes.


## *caret* packagev: `nnet` model

With the `nnet` function, we don't need to transdorme our target data to binary type. Also, we can use `createDataPartition` function to split the data in train and test.

```{r}
library(nnet)

set.seed(params$seed.train)
# We wish 75% for the trainset 
inTrain <- createDataPartition(y=data$type, p=0.66666666666666667, list=FALSE)
#dim(inTrain)

# normalized dataset
data_nrm_net <- cbind(data_nrm[,1:10],type=data[,11])

train.set_net <- data_nrm_net[inTrain,]
test.set_net  <- data_nrm_net[-inTrain,]

nrow(train.set_net)/nrow(test.set_net) # should be around 2
```


**3-fold crossvalidation**

Also, we are going to check the performance with 3-fold crossvalidation

```{r}

set.seed(1234567)

#3-crossvalidation model
model_net3_cross <- train(type ~ ., train.set_net, method='nnet', 
               trControl= trainControl(method='cv', number=3), 
               tuneGrid= NULL, tuneLength=10 ,trace = FALSE)

plotnet(model_net3_cross, alpha=0.6)
summary(model_net3_cross)
prediction_net3_cross <- predict(model_net3_cross, test.set_net[-11])   # predict
table(prediction_net3_cross, test.set_net$type)                         # compare

# predict can also return the probability for each class:
prediction_net3_cross_pro <- predict(model_net3_cross, test.set_net[-11], type="prob")  
head(prediction_net3_cross_pro)
```



# Support Vector Machine (SVM)

A **Support Vector Machine (SVM)** can be imagined as a surface that creates a boundary between points of data plotted in multidimensional that represent examples and their feature values. The goal of a SVM is t create a flat boundary called **hyperplane**, which divides the space to create fairly homogeneus partitions on either side. In this way, the SVM learning combines aspects of both the instance-based nearest neighbor learning and the linear regression modeling. The combination is extremly powerful, allowing SVMs to model highly complex relationships.

| **Strengths**    | **Weaknesses**  | 
| ----------------------------------- |:-----------------------------------|
|- Can be used for classification or numeric prediction problems |- Finding the best model requires testing of various combinations of kernels and model parameters |
| - Not overly influenced by noisy data and not very prone to overfitting | - Can be slow to train, particularly if the input dataset has a large number of features or examples |
| - May be easier to use than neural networks, particularly due to the existence of several well-supported SVM algorithms | - Results in a complex black box model that is difficult, if not impossible, to interpret
| - Gaining popularity due to its high accuracy and high-profile wins in data mining competitions

## Step 3 - Training a model on the data

We are going to use the  `ksvm()` function from `kernlab` package to train the SVM model. We are going to build a linear model with the kernel value `vanilladot`


```{r}
set.seed(1234567)

lm <- ksvm( type ~ ., data = data_svm.train,kernel = "vanilladot")
lm
```

## Step 4 - Evaluating model performance

The `predict()` function allows us to use the classification model to make predictions on the testing dataset `data_svm.test`


```{r}
pre_lm <- predict(lm, data_svm.test)
pre_lm
```

Let's see the model performance with `ConfusionMatrix()` function

```{r}
cm_lm <- confusionMatrix(pre_lm, data_svm.test$type)
cm_lm
```

Using the support vector machine model with a lineal kernel, we obtain `r sum(cm_lm$t)`, with `r cm_lm$t[1,1]` true positives in the case of ALL for example. With this algorithm, we obtain `r round(cm_lm[["overall"]][["Accuracy"]], 3)`precision and a `r round(cm_lm[["overall"]][["Kappa"]], 7)` kappa value.


## Step 5 - Improvement the model

We are going to do the same but with a Gaussian kernel `rbfdot` to try to improve the model performance

```{r}
set.seed(1234567)

gs <- ksvm(type ~ ., data = data_svm.train, kernel='rbfdot')
gs
```

```{r}
pre_gs <- predict(gs, data_svm.test)
pre_gs
```

```{r}
cm_gs <- confusionMatrix(pre_gs, data_svm.test$type)
cm_gs
```

Using the support vector machine model with a gaussian kernel, we obtain `r sum(cm_gs$t)`, with `r cm_gs$t[1,1]` true positives in the case of ALL for example. With this algorithm, we obtain `r round(cm_gs[["overall"]][["Accuracy"]], 3)`precision and a `r round(cm_gs[["overall"]][["Kappa"]], 7)` kappa value.

In general, both methods are efficient, but in this case the linear model has better performance with this data

### SVM `svmLinear`: 3-fold crossvalidation

```{r}

# model 3-crossvalidation 
set.seed(1234567)
modelb <- train(type ~ ., data_svm.train, method='svmLinear', 
               trControl= trainControl(method='cv', number=3), 
                tuneGrid= NULL, trace = FALSE)


modelb
predictionb <- predict(modelb, data_svm.test)                           # predict
resb <- table(predictionb, data_svm.test$type)                             # compare

confusionMatrix(resb)   
```


# Discussion 

In the case of this dataset, we have five type of diagnostics so it doesn't make sense to award a positive category. In general, all the method have had a good performance, to understand better the reasons of the small changes we should analyze the preprocessing steps of the data. Due to the high different variables, it make sense the neural network with five hidden nodes has have a better performance instead of one hidden node model.

We know the SVM model is very powerfull methods, in both cases it has have really good performance. Also, we didn't need to performn additional changes in the data, so this is a very considerable advantaje but also it increments the "black box" concept. In addition, the SVM seems to be the best method for these data.

In both cases we developt a crossvalidation mehotd we have very nice results. This is an example of another type of sampling we can apply to our data. But it has more computational work, it is a good option if your data is not very extensive.


