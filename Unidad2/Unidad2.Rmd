---
title: "K-NN"
author: "Laura Sudupe Medinilla"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
bibliography: scholar.bib
lang: en # language,  en: english (default), es: espa?ol, ca: catalan, ..
---

```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = TRUE)

```

```{r libraries, include=FALSE}
# Install packages
# Load packages
# ...

library(knitr)
```


```{r input, include=FALSE}
# Input / Output variables
# Tuning parameters
# ...
file1 <- "usedcars.csv"

```

# Exploring and preparing the data

Import the CSV data file to the `cancer_mama` dataframe.

```{r}
cancer_mama <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
```

Using the `str(cancer_mama)`command, we can see the data structure. 

```{r}
str(cancer_mama)
```
We can see the stdy has `r ncol(cancer_mama)` examples and `r nrow(cancer_mama)`
features. The first feature `id` is a unique identifier for each patient in the data, we will
exclude it from the model.
```{r}
cancer_mama <- cancer_mama[-1]
```

The variable `diagnosis` is going to be our label, the oucome we hope to predict.
This feature indicates whether the exmaple is from a benign or malignant mass.
With the `r table()` output we can see that `r table(cancer_mama$diagnosis == 'B`)
are benign while `r table(cancer_mama$diagnosis == "M")` are malignant.
```{r}
table(cancer_mama$diagnosis)
```
Many R machine learning classifiers require the target feature is coded as a 
factor, so we will recode `diagnosis` feature.
```{r}
cancer_mama$diagnosis <- factor(cancer_mama$diagnosis, levels = c("B", "M"),
                                labels = c("Benign", "Malignant"))
```
Let´s check the `Benign` and `Malignant` percentages with `r prop.table()`
```{r}
round(prop.table(table(cancer_mama$diagnosi)) * 100, digits=1)
```
All the remaining features are numeric, they consist of three different 
measurements of ten characteristics. We will take a closser look of three of 
these features
```{r}
summary(cancer_mama[c("radius_mean", "area_mean", "smoothness_mean")])
```
The distance calculation for k-NN is heavily dependent upon the measurement 
sclae of the input features. Since `smoothness_mean` ranges from `r round(min(cancer_mama$smoothness_mean), 2)` 
to `r round(max(cancer_mama$smoothness_mean), 2)` and `area_mean` ranges from 
`r round(min(cancer_mama$area_mean), 2)` to `r round(max(cancer_mama$area_mean), 2)`,
the impact of area is going to be much larger than smoothness in the distance
calculation. This cpuld potentially cause problems for our classifier, so let´s
apply normalization to rescale the features to a standard range of values.

# Transformation - normalizing numeric data




























