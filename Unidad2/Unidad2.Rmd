---
title: "K-NN"
author: "Laura Sudupe Medinilla"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
bibliography: scholar.bib
lang: en # language,  en: english (default), es: espa?ol, ca: catalan, ..
---

```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = TRUE)

```

```{r libraries, include=FALSE}
# Install packages
# Load packages
# ...

library(knitr)
```


```{r input, include=FALSE}
# Input / Output variables
# Tuning parameters
# ...
file1 <- "usedcars.csv"

```

# Exploring and preparing the data

Import the CSV data file to the `cancer_mama` dataframe.

```{r}
cancer_mama <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
```

Using the `str(cancer_mama)`command, we can see the data structure. 

```{r}
str(cancer_mama)
```
We can see the stdy has `r ncol(cancer_mama)` examples and `r nrow(cancer_mama)`
features. The first feature `id` is a unique identifier for each patient in the data, we will
exclude it from the model.
```{r}
cancer_mama <- cancer_mama[-1]
```

The variable `diagnosis` is going to be our label, the oucome we hope to predict.
This feature indicates whether the exmaple is from a benign or malignant mass.
With the `r table()` output we can see that `r table(cancer_mama$diagnosis == 'B`)
are benign while `r table(cancer_mama$diagnosis == "M")` are malignant.
```{r}
table(cancer_mama$diagnosis)
```
Many R machine learning classifiers require the target feature is coded as a 
factor, so we will recode `diagnosis` feature.
```{r}
cancer_mama$diagnosis <- factor(cancer_mama$diagnosis, levels = c("B", "M"),
                                labels = c("Benign", "Malignant"))
```
Let´s check the `Benign` and `Malignant` percentages with `r prop.table()`
```{r}
round(prop.table(table(cancer_mama$diagnosi)) * 100, digits=1)
```
All the remaining features are numeric, they consist of three different 
measurements of ten characteristics. We will take a closser look of three of 
these features
```{r}
summary(cancer_mama[c("radius_mean", "area_mean", "smoothness_mean")])
```
The distance calculation for k-NN is heavily dependent upon the measurement 
sclae of the input features. Since `smoothness_mean` ranges from `r round(min(cancer_mama$smoothness_mean), 2)` 
to `r round(max(cancer_mama$smoothness_mean), 2)` and `area_mean` ranges from 
`r round(min(cancer_mama$area_mean), 2)` to `r round(max(cancer_mama$area_mean), 2)`,
the impact of area is going to be much larger than smoothness in the distance
calculation. This cpuld potentially cause problems for our classifier, so let´s
apply normalization to rescale the features to a standard range of values.

# Transformation - normalizing numeric data

To normalize these features, we need to create a `normalize()` function. 
```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
```
The ´lapply()´ function take a list and applies a specified function to each list 
element. As a data frame is a list of equal-length vectors, we can use ´lapply()´
to apply ´normalize()´ to each feature in the data frame. The final step is to 
convert the list returned by `lapply()` to a data frame, using the ´as.data.frame()´
function
```{r}
cancer_mama_n <-as.data.frame(lapply(cancer_mama[2:31], normalize))
```
Check the transformation was applied correctly
```{r}
summary(cancer_mama_n[c("radius_mean", "area_mean", "smoothness_mean")])
```

# Data preparation - creating training and test datasets

We can simulate unkown label data by dividing our data into two portions: a
training dataset that will be used to build the k-NN model and a test dataset
that will be used to estimate the predictive accuracy of the model. We will use
the first 469 records for the training dataset and the remaining 100 to simulate
new patients.We can do these because the data is already randomly ordered.
```{r}
cancer_train <- cancer_mama_n[1:469,]
cancer_test <- cancer_mama_n[470:569,]
```

The next step is to exclude the target variable ´diagnosis´. For training the 
k--NN model, we will need to store these class labels in factor vectors, split 
between the training and test datasets
```{r}
cancer_train_labels <- cancer_mama[1:469, 1]
cancer_test_lables <- cancer_mama[470:569, 1]
```
With these code we create the vectors ´cancer_train_labels´ and ´cancer_test_labels´.
We will use these in the next steps of training and evaluating our classifier.

# Training a model on the data

For the k-NN algorithm, the training phase actually involves no model building; 
the process of training simply involves storing the input data in a structured 
format.

To classify our test instances, we will use a k-NN implementation from the ´r class´
package, which provides a set of basic R functions for classification. 

```{r}
#install.packages("class")
library("class")
```
The ´r knn()´ function in the ´r class´ package provides a standard, classic 
implementation of the k-NN algorithm. For each instance in the test data, the 
function will identify the K-Nearest Neighbors, using Eucledian distance, where 
*k* is a user-specified number. The test instance is classified by taking a "vote"
among the k-Neares Neighbors- specifically, this involves assigning the class of
the majority of the *k* neighbors.
```{r}
cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
                        cl = cancer_train_labels, k = 21)
```
The ´r knn()´ function returns a factor vector of predicted labels for each of 
the examples in the ´test´ dataset, which we hace assigned to ´cancer_test_pred´.


# Evaluating model performance


















