---
title: "PEC1: Algorithm K-NN"
author: "Laura Sudupe Medinilla"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  pdf_document:
    toc: TRUE
  html_document:
    toc: TRUE
knit: rmarkdown::render("PEC1: Algorithm K-NN", c("pdf_document", "html_document"),output_file = c("PEC1: Algorithm K-NN","PEC1: Algorithm K-NN"))
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NULL)
```

```{r load_libraries, include=FALSE}
require(knitr)
require(class)
require(caret)
require(caTools)
require(gmodels)
require(base)
require(data.table)
```

```{r}
install.packages("data.table")
```


# The kNN Algorithm

In this markdown we are going to learn about classification using k-NN. Unlike
many classification algorithms, k-NN does not do any learning. It simply stores 
the training data verbatim. Unlabeled test examples are then matched to the most 
similar records in the training set usin a distance function, and the unlabeled
example is assignaed the label of its neighbors.

The strengths and weaknesses of this algorithm are as follows:

| **Strengths**    | **Weaknesses**  | 
| ------------------------ |:------------------------------------------------------- |
|* Simple and effective     |* Does not produce a model, which limits the ability to find novel insights in relationships among features |
| * Makes no assumptions about the underlying data distribution | * Slow classification phase |
| * Fast training phase | * Requires a large amount of memory  
|    | * Nominal features and missing data require additional processing |



# Step 1 - Collecting data

We will utilize the "Protein Secondary Structure" from the Brookhaven National Laboratory (USA).This data includes the protein secondary structures from 101 representative proteins.


```{r data, include=FALSE}
amino <- read.csv("data4.csv", head=TRUE, sep= ";")
amino_one_hot <- read.csv("oh_enc.csv", stringsAsFactors = FALSE)
```


# Step 2 - Exploring and preparing the data

We are going to take a look to our data with `str(amino)`. 
```{r, echo=FALSE}
str(amino, list.len=6)
```
The data is structured with `r nrow(amino)` sequences with `r ncol(amino)` amino acids. We want to see which is the central amino acid structure type. The `table()` output indicates that `r as.numeric(table(amino$V18))[1]` aminos has **coil** structure, `r as.numeric(table(amino$V18))[2]` has **beta-sheet** structure and `r as.numeric(table(amino$V18))[3]` has **alpha-helix** structure.

```{r}
table(amino$V18)
```
We can label the data and check the structure porcentages with `prop.table()`
```{r}
amino$V18 <- factor(amino$V18, levels = c("_", "e", "h"),
              labels=c("coil","beta-sheet","alpha-sheet"))
round(prop.table(table(amino$V18)) * 100, digits = 1)
```
## Data preparation 

### One-hot transformation 

### Creating training and test datasets

We are going to divide our data into two portions: a training dataset that will be used to build the KNN model and a test dataset that will be used to estimate the predictive accuracy of the model. 
```{r}
set.seed(123)

train_indx <- sample(x= 1:nrow(amino_one_hot),
                     size= 0.67*nrow(amino_one_hot),
                     replace=FALSE)
train_data <- amino_one_hot[train_indx,]
test_data <- amino_one_hot[-train_indx,]

label_train_idx <- sample(x=1:nrow(amino),
                          size=0.67*nrow(amino),
                          replace= FALSE)
train_label <- amino$V18[label_train_idx]
test_label <- amino$V18[-label_train_idx]


```


# Step 3 - Training a model on the data

For the classification, we will use a kNN implementation from the `class` package. We use the `knn()` function to classify the test data
```{r}
amino_test_pred <- knn(train= train_data, test= test_data,
                       cl=train_label, k=21)

```

The `knn()` function returns a factor vector of predicted labels for each of the examples in the test dataset, which we have assigned to `amino_test_pred`.


# Step 4 - Evaluating model performance

We need to evaluate how well the predicted classes in the `amino_test_pred` vector match up with the known values in the `test_label` vector.
To these purpose, we can use the `CrossTable()` function. We will create a cross tabulation indicating the agreement between two vectors.


```{r}
CrossTable(x=test_label, y=amino_test_pred, prop.chisq=FALSE)
```
The cell percentage in the table indicate the portion of values that fall into four categories. In the top-left, these `r conf.mat$t[1,1]` of `r sum(conf.mat$t)`, cell we have the number of values the KNN algorithm correctly identified as coil. In the diagonal we have the true posiive values, for example, these `r conf.mat$t[3,3]` of `r sum(conf.mat$t)`, shows values the KNN correctly identified as alpha-sheet.

In the case of `r conf.mat$t[2,1]` examples are false negatives, the predicted value were coil but the structure were beta-sheet. The same happens with `r conf.mat$t[3,1]` of `r sum(conf.mat$t)`, the predicted values were coil but the structure were alpha-sheet. 

On the other side, a total of `r conf.mat$t[2,3]` out of `r sum(conf.mat$t)` were incorrectly classified by the KNN algorithm like alpha-sheet.


# Step 5 - Improving the model performance

We will try several different values for **k**, (k = 1, 3, 5, 7, 11), to examine the performance. Using the previous test and training datasets, the same `r tr` records were classified. The number of false negatives and false positives are shown for each iteration.

```{r, include=FALSE}

ks <- c(1, 3, 5, 7, 11)
resum <- data.frame(ks, FN=NA, FP=NA, mal_clas=NA)

j <- 0
for (i in ks){
  j <- j +1
  amino_test_pred <- knn(train = train_data, test = test_data, cl = train_label, k=i)
  conf.mat <- CrossTable(x = test_label, y = amino_test_pred, prop.chisq=FALSE)
  
  resum[j,2:4] <- c(conf.mat$t[2,1], conf.mat$t[1,2], ((conf.mat$t[1,2]+conf.mat$t[2,1])/sum(conf.mat$t))*100)
}

```

```{r, echo=FALSE}
kable(resum, col.names=c("k value", "# false negatives", 
      "# false positives", "% classified Incorrectly"),
      align= c("l","c","c","c"))
```
The 11NN aproach was able yo avoid a lot of false positives at the expense of adding false negatives. In these cases, the 1NN aproach has the higher porcentage os classified incorrectly.



# Step 7 - Prove our KNN classifier with different labels


We know the alpha-helix and beta-sheet are both non-coil structures. We are going to performe the same algorithm for these labels.

```{r}
# copy amino dataframe 
aminocoil <- copy(amino)

#re-label and check the porcentages

aminocoil$V18 <- factor(amino$V18, levels = c("coil", "beta-sheet", "alpha-sheet"),
              labels=c("coil","no-coil","no-coil"))

round(prop.table(table(aminocoil$V18)) * 100, digits = 1)


```

```{r}

```




















