---
title: 'Classification using Naive Bayes'
author: Laura Sudupe Medinilla
subtitle: '`r params$subtitulo`'
date: '`r format(Sys.Date(),"%e de %B, %Y")`' 
# date: \today  (solo para pdf)
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float: true
    theme: united
    higlight: tango
  pdf_document:
    keep_tex: yes
    toc: yes
nocite: |
  @lantz2015machine
header-includes: \usepackage[spanish]{babel}
params:
  file1: genotype.csv
  file2: flowering_time.csv
  folder.data: ./floweringTime
  p.train: 0.6666667
  subtitulo: Predict flowering type depending plant genotype
  valor.seed: 1234
bibliography: pec1.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NULL)
options(width=90)
```


```{r packages, message=FALSE, echo=FALSE, warning=FALSE}
libraries <- c("e1071", "gmodels",  "ROCR", "caret")
check.libraries <- is.element(libraries, installed.packages()[, 1])==FALSE
libraries.to.install <- libraries[check.libraries]
if (length(libraries.to.install!=0)) {
  install.packages(libraries.to.install)
}
library("e1071")
library("gmodels")
library("ROCR")
library("caret")
```


\pagebreak

# Naive Bayes Algorithm 

The *Naive Byes* algorithm describes a simple method to aplly Bayes theorem to classification problems. Althopugh it is not the only machine learning method that utilizes Bayesian methods, it is the most common one. This is particularly true for text classification, where it has become the de facto standard. The strengths and weaknesses of this algorithm are as follows:


| **Strenghts**    | **Weaknesses**  | 
| ----------------------------- |:------------------------------------------- |
| * Simple, fast, and very effective | * SRelies on an often-faulty assumption of equally important and independent features|
| * Does well with noisy and missing data  |  * Not ideal for datasets with many numeric features |
| * Requires relatively few examples for training, but also works well with very large numbers of examples  | * Estimated probabilities are less reliable than the predicted classes |
| * Easy to obtain the estimated probability for a prediction |  | 

# Step 1 - Collecting the data

```{r, echo=FALSE}
file1 <- "genotype.csv"
file2 <- "flowering_time.csv"

genotype <- read.csv(file1, header=FALSE)
names(genotype) <- paste("gtype",1:ncol(genotype),sep=".")

flowering_time <- read.csv(file2,header= FALSE, col.names= "Flow_time", 
                           stringsAsFactors=TRUE)

```

This algorithm objetive is to predict the flowering type (fast or slow) in function of the flower genotype.

We know the genotype of `r nrow(flowering_time)` flowers and the flowering day. We have `r ncol(genotype)` genotypes as predictor features. The genotype states are 0, 1 or 2, which belong to dominant homozygote, heterozygote and recessive homozigote.


# Step 2 - Exploring and preparing the data

The first step towards constructing our classifier involves processing the raw data for analysis.

Using the `str()` function, we see our data structure

```{r, echo=FALSE}
# examine the structure of R objecte
str(genotype, list.len=5)
```
The element are numeric, it would be better to convert it into a factor

```{r, echo=FALSE}

genotype_f <-data.frame(lapply(genotype,as.factor))
```

Check again the structure
```{r, echo=FALSE}
# 
str(genotype_f, list.len=5)
```
We need to create a new feature, *fast* or *slow* flowering. If the number of days is higher or same to `r (b)` days, we codifice like 1, else, 0.

```{r, echo=FALSE}
b <- 40
flowering_factor <- as.factor(ifelse(flowering_time>b,1,0))
flowering_factor <- factor(flowering_factor, labels = c("fast", "slow"))

```

Take a look to the plants type numbers

```{r, echo=FALSE}
table(flowering_factor)
```

# Step 3 - Entrenamiento de un modelo con los datos

A continuaci?n, preparamos nuestro *training dada set* que corresponde a `r round((p <- params$p.train),2)` del total y nuestro *test data set* que es el resto:   

```{r}
set.seed(params$valor.seed)
train <- sample(nrow(genotype_f),floor(nrow(genotype_f)*p))
length(train)

mydata_training<-genotype_f[train,]
mydata_test<-genotype_f[-train,]
class_training<-flowering_time_binary[train]
class_test<-flowering_time_binary[-train]
```

Observar que se inicializa la semilla para poder repetir la misma serie cada vez.

Una vez hecha la partici?n se puede entrenar el algoritmo.

```{r}
#library(e1071)
mydata_clsf <- naiveBayes(mydata_training, class_training, laplace=0)
```

El contenido del objecto R resultado del entrenamiento contiene las probabilidades condicionadas de cada categoria seg?n el tipo de floraci?n.

```{r}
mydata_clsf$tables[1:4]
```


# Paso 4 - Evaluaci?n del comportamiento del modelo 

Aplicamos la funci?n `predict` del algoritmo con los datos de test para hacer su predicci?n:

```{r}
test_pred <- predict(mydata_clsf, mydata_test)
```

Ahora miramos los resultados en una *Cross Table* usando la funci?n `CrossTable` del package `gmodels`:

```{r}
#library(gmodels)
CrossTable(x =test_pred , y = class_test , prop.chisq=FALSE)
```

o con la funci?n `confusionMatrix` del package `caret`.

```{r}
#require(caret,quietly = TRUE)
confusionMatrix(test_pred,class_test,positive='1')
```


# Paso 5 - Mejora del comportamiento del modelo

Ahora se prueba a entrenar el modelo aplicando la opci?n `laplace = 1`:

```{r}
mydata_clsf2 <- naiveBayes(mydata_training, class_training, laplace=1)
```

y se hace la predicci?n:

```{r}
test_pred2 <- predict(mydata_clsf2, mydata_test)
```


Ahora se evalua el modelo con la funci?n `CrossTable` del package `gmodels`:

```{r}
#library(gmodels)
CrossTable(x =test_pred2 , y = class_test , prop.chisq=FALSE)
```

o con la funci?n `confusionMatrix` del package `caret`.

```{r}
#require(caret,quietly = TRUE)
confusionMatrix(test_pred2,class_test,positive='1')
```

Se observa que el resultado es un poco peor que con la condici?n ``laplace = 0`. Por tanto, no tiene sentido aplicar `laplace = 1` para mejorar el modelo.

# Curvas ROC

Se presenta las curvas ROC para el modelo de Naive Bayes con `laplace=0` y `laplace= 1`.

## Caso `laplace=0`

El primer paso es obtener las probabilidades de tipo de floraci?n (lenta/r?pida) para cada planta de los datos test.

```{r}
test_pred <- predict(mydata_clsf, mydata_test, type="raw")

tail(test_pred)
```

Con la informaci?n de las probabilidades de la clase positiva (1) se construye la curva ROC.

```{r, include=FALSE}
#require(ROCR,quietly=TRUE)
```

```{r}
pred <- prediction(predictions= test_pred[,2], labels=class_test)
perf <- performance(pred, measure="tpr", x.measure="fpr")
#unlist(perf@alpha.values)


plot(perf, main= "ROC curve", col= "blue", lwd=3, colorize=TRUE)
abline(a=0, b= 1, lwd= 2, lty = 2)
perf.auc <- performance(pred, measure ="auc")
# see http://rocr.bioinf.mpi-sb.mpg.de/

#str(perf)
```

El area bajo la curva es **`r unlist(perf.auc@y.values)`**.

## Caso `laplace=1`

El primer paso es obtener las probabilidades de tipo de floraci?n (lenta/r?pida) para cada planta de los datos test.

```{r}
test_pred2 <- predict(mydata_clsf2, mydata_test, type="raw")

tail(test_pred2)
```

Con la informaci?n de las probabilidades de la clase positiva (1) se construye la curva ROC.


```{r}
pred2 <- prediction(predictions= test_pred2[,2], labels=class_test)
perf2 <- performance(pred2, measure="tpr", x.measure="fpr")
#unlist(perf@alpha.values)


plot(perf2, main= "ROC curve", col= "blue", lwd=3, colorize=TRUE)
abline(a=0, b= 1, lwd= 2, lty = 2)
perf2.auc <- performance(pred2, measure ="auc")


#str(perf)
```

El area bajo la curva es **`r unlist(perf2.auc@y.values)`**.

#Referencias
